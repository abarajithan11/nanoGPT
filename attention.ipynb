{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "B = 4  # batch, independant sets processed in parallel for efficiency\n",
    "T = 8  # time/block/context, window of tokens considered\n",
    "C = 32 # in_channels, each token is \"embedded\" into a vector of size C\n",
    "H = 16 # out_channels, seperate similar operations on same vectors\n",
    "\n",
    "x = torch.randn(B,T,C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked Self Attention (Goal)\n",
    "\n",
    "Attention(x) = V.σ(QK/sqrt(H)), x:(B,T,C), y: (B,T,H)\n",
    "\n",
    "|         | Info                                                                                                                             |\n",
    "| ------- |--------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| x       | private information of token                                                                                                     |  \n",
    "| q       | each token generates a query vector , [I am a vowel in position 8 looking for consonents upto position 4]                        |  \n",
    "| k       | each other token generates a key vector, what information I have [I am a consonent in position 3]                                |\n",
    "| w=qk    | affinity - those two tokens find each other, affinity at the intersection will be very high (I am interested in these positions) |\n",
    "| v       | value vector, what information I am willing to provide                                                                           |\n",
    "| y=vσ(w) | accumulate all the information from interested positions to me                                                                   |\n",
    "\n",
    "\n",
    "### Notes\n",
    "\n",
    "- Attention is a communication mechanism\n",
    "- A directed graph of T nodes, each node being a token position, and contains info as a vector of H size\n",
    "- T nodes aggregate infromation as a weighted sum from all nodes\n",
    "- Data dependant, the data stored in the nodes change with time\n",
    "- For autogression,\n",
    "  - 1-th node gets only from itself\n",
    "  - 2-nd node gets from 1,2\n",
    "  - T-th node gets from everyone\n",
    "- Nodes have no notion of space / ordering. So we need to add postional embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "key   = nn.Linear(C, H, bias=False)\n",
    "query = nn.Linear(C, H, bias=False)\n",
    "value = nn.Linear(C, H, bias=False)\n",
    "\n",
    "# x          # (B,T,C)\n",
    "k = key(x)   # (B,T,H)\n",
    "q = query(x) # (B,T,H)\n",
    "wei = q @ k.transpose(-2,-1) # (B,T,H) @ (B,H,T) = (B,T,T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))               # for decoder: lower triangular matrix (including diagonal)\n",
    "wei  = wei.masked_fill(tril==0, float('-inf'))   # Replace upper triangular of wei with -inf\n",
    "wei /= H**0.5                                    # scaling, bring variance to 1, to prevent softmax clipping\n",
    "wei  = F.softmax(wei, dim=-1)                    # -inf -> 0, rest normalized to 1\n",
    "\n",
    "v = value(x)   # (B,T,H)\n",
    "out = wei @ v  # (B,T,T) @ (B,T,T) = (B,T,H)\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder vs Decoder\n",
    "\n",
    "- Encoder\n",
    "  - no triangular mask (it gathers data from both directions)\n",
    "  - eg. translation, sentiment analysis\n",
    "- Decoder\n",
    "  - triangular mask (gathers data from past & present only)\n",
    "  - predicts next word\n",
    "  - \"autoregressive\", P(next_word) = P(this_word|past_words) * P(prev_word|words_before)...\n",
    "\n",
    "Attention = V*softmax(QK.T/sqrt(H))\n",
    "\n",
    "- Self-Attention: Q,K,V come from X\n",
    "- Cross-attention: \n",
    "  - query from x, keys & values come from different place. \n",
    "  - eg: English -> French, french (query) searches in English (key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
